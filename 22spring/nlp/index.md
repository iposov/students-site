# Статистические методы обработки языка и речи

1. Примеры работы с nltk и рисование графиков (helloworld.ipynb)[http://nbviewer.jupyter.org/github/iposov/students-site/blob/master/22spring/nlp/helloworld.ipynb]
2. Jupyter, закон Ципфа: [jupyter-zipf.pdf](jupyter-zipf.pdf)
3. Дополнительные задания по закону Ципфа. N-грамм модели: [zipf-ngram.pdf](zipf-ngram.pdf)
4. Numpy, Линейная регрессия: (numpy-linear-regression.ipynb)[http://nbviewer.jupyter.org/github/iposov/students-site/blob/master/22spring/nlp/numpy-linear-regression.ipynb]
5. N-грамм модели в NLTK: (ngram.ipynb)[http://nbviewer.jupyter.org/github/iposov/students-site/blob/master/22spring/nlp/ngram.ipynb]
6. N-грамм модели и оценка модели: [ngrams.pdf](ngrams.pdf)
7. Классификация методом наивного байеса: [naive-bayes.pdf](naive-bayes.pdf), [naive-bayes.ipynb](http://nbviewer.jupyter.org/github/iposov/students-site/blob/master/22spring/nlp/naive-bayes.ipynb)
8. [Кластеризация](clusterization.pdf) — пока без примеров кода
9. [Word2Vec](word2vec.pdf) — пока без примеров кода

## Работа с текстом

- Будем работать с текстами на русском
- Найденные тексты сохраняйте себе на диск в plain text, если у вас другой формат, самостоятельно разберитесь в библиотеках python, которые этот формат читают.
- Выкладывать в общий доступ чужие тексты нельзя (чаще всего). Храните их у себя на диске, выкладывайте в какое-то приватное облако, можете посылать в teams.
- Я буду запускать ваши программки на своих текстах, при необходимости, вы дадите свой текст.
- В блокнотах результаты будут видны по результатам обработки вашего текста.
- внимательно с кодировками

## Задания
### Закон Ципфа
* Подготовьте корпус
* Вычислите частотный словарь и постройте график логарифма частоты от логарифма ранка
* Аналогично, но приведите слова к начальной форме
* Вычислите линию регрессии, распечатайте ее коэффициенты и нарисуйте её на графике.

### N-грамм модели
* Подготовьте корпус
* Прочитайте его, разбив на предложения, а предложения на слова. Используйте встроенные в NLTK токенизаторы предложений и слов.
* Постройте модели MLE, Lindstone с $\lambda=1$ и $\lambda=0.1$.
* Перепишите код так, чтобы корпус разбивался на две части, примерно 80% в одной (обучающей) и 20% в другой (тестовой), обучите модели на обучающей части корпуса, посчитайте перплексити на второй части.
* Добавьте модель Kneyser-Ney, разбейте исходный корпус дополнительно еще и на валидационную часть, используйте ее, чтобы подобрать оптимальные параметры моделей.

### Наивный Байес

* Возьмите [корпус отзывов Кинопоиска](https://www.kaggle.com/datasets/mikhailklemin/kinopoisks-movies-reviews), оставьте из него только положительные и отрицательные отзывы, обучите на части этого корпуса классификатор Наивного Байеса. На оставшейся части корпуса оцените качество кластеризации. Выведите самые полезные признаки

### Кластеризация

* Найдите интересный текст на русском языке, разбейте его на слова. Выберите размер окна, например, w = 11, т.е. по 5 слов слева и справа от каждого слова. Создайте из контекстов вектора tf-idf. Пример создания tf-idf векторов по ссылке [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)
* Вас интересуют контексты только определенного слова, например, "кот", "замок", "банан+кот" (объединенное слова), оставьте только эти контексты.
* Уменьшите размерность векторов с помощью метода главных компонент до 20 [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html), перед этим [нормализуйте данные](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize) — наш собственный пример кода пока потерялся, но остался на записи. 
* Кластеризуйте контексты, посмотрите, какие классы смыслов слов получились.
* Визуализируйте кластеризацию за счет уменьшения размерности еще раз до 2.

### Word2Vec

Получите с сайта [https://rusvectores.org/ru/](https://rusvectores.org/ru/) одну из word2vec моделей. Возьмите текст, разбейте его на слова, получите матрицу: строки соответствуют словам текста, внутри строки word2vec представление слова. Запустите KMeans кластеризацию на этой матрице. Т.е. кластеризуйте слова по смыслу. Уменьшите размерность с 300 до 2 и нарисуйте все точки на плоскости, желательно разные кластеры — разными цветами.
